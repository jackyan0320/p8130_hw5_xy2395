p8130\_hw5\_xy2395
================
Jack Yan
11/30/2018

Dataset Description
===================

`state.x77` is a matrix with 50 rows and 8 columns giving the following statistics in the respective columns:

-   Population: population estimate as of July 1, 1975.
-   Income: per capita income (1974).
-   Illiteracy: illiteracy (1970, percent of population).
-   Life Exp: life expectancy in years (1969–71).
-   Murder: murder and non-negligent manslaughter rate per 100,000 population (1976).
-   HS Grad: percent high-school graduates (1970).
-   Frost: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city.
-   Area: land area in square miles.

``` r
# Load the data
state_df = 
  state.x77 %>% 
  as.tibble() %>% 
  janitor::clean_names() %>% 
  select(life_exp, everything())
  
# state_df %>% skimr::skim()
```

Problem 1: Exploratory Data Analysis
====================================

Descriptive Statistics
----------------------

Descriptive Statistics for all the variables are shown below. All the variables are continuous.

``` r
# Build a function to generate descriptive statistics for continuous variables
summary_continuous = function(variable){
  data_frame(
    mean = mean(variable),
    sd = sd(variable),
    median = median(variable),
    maximum = max(variable),
    minimum = min(variable),
    IQR = IQR(variable)
  )
}

# Generate descriptive statistics
map(state_df, summary_continuous) %>% 
  bind_rows() %>% 
  mutate(variable = names(state_df)) %>% 
  select(variable, everything()) %>% 
  knitr::kable(digits = 2, 
               caption = "Descriptive statistics of continuous variables")
```

| variable   |      mean|        sd|    median|   maximum|  minimum|       IQR|
|:-----------|---------:|---------:|---------:|---------:|--------:|---------:|
| life\_exp  |     70.88|      1.34|     70.67|      73.6|    67.96|      1.78|
| population |   4246.42|   4464.49|   2838.50|   21198.0|   365.00|   3889.00|
| income     |   4435.80|    614.47|   4519.00|    6315.0|  3098.00|    820.75|
| illiteracy |      1.17|      0.61|      0.95|       2.8|     0.50|      0.95|
| murder     |      7.38|      3.69|      6.85|      15.1|     1.40|      6.32|
| hs\_grad   |     53.11|      8.08|     53.25|      67.3|    37.80|     11.10|
| frost      |    104.46|     51.98|    114.50|     188.0|     0.00|     73.50|
| area       |  70735.88|  85327.30|  54277.00|  566432.0|  1049.00|  44177.25|

Also show the correlation matrix to check potential correlations. `life_exp` and `murder` are highly correlated. `illiteracy` and `murder` are correlated as well.

``` r
cor(state_df) %>% 
   knitr::kable(digits = 2)
```

|            |  life\_exp|  population|  income|  illiteracy|  murder|  hs\_grad|  frost|   area|
|------------|----------:|-----------:|-------:|-----------:|-------:|---------:|------:|------:|
| life\_exp  |       1.00|       -0.07|    0.34|       -0.59|   -0.78|      0.58|   0.26|  -0.11|
| population |      -0.07|        1.00|    0.21|        0.11|    0.34|     -0.10|  -0.33|   0.02|
| income     |       0.34|        0.21|    1.00|       -0.44|   -0.23|      0.62|   0.23|   0.36|
| illiteracy |      -0.59|        0.11|   -0.44|        1.00|    0.70|     -0.66|  -0.67|   0.08|
| murder     |      -0.78|        0.34|   -0.23|        0.70|    1.00|     -0.49|  -0.54|   0.23|
| hs\_grad   |       0.58|       -0.10|    0.62|       -0.66|   -0.49|      1.00|   0.37|   0.33|
| frost      |       0.26|       -0.33|    0.23|       -0.67|   -0.54|      0.37|   1.00|   0.06|
| area       |      -0.11|        0.02|    0.36|        0.08|    0.23|      0.33|   0.06|   1.00|

Plots
-----

First, plot the highly correlated variables.

``` r
plot(state_df)
```

![](p8130_hw5_xy2395_files/figure-markdown_github/unnamed-chunk-4-1.png)

``` r
murder_illiteracy_points = 
state_df %>% 
  ggplot(aes(x = murder, y = illiteracy))+
    geom_point()
murder_lifeexp_points = 
state_df %>% 
  ggplot(aes(x = murder, y = life_exp))+
    geom_point()
murder_illiteracy_points + murder_lifeexp_points
```

![](p8130_hw5_xy2395_files/figure-markdown_github/unnamed-chunk-4-2.png)

Then check the normality of the outcome, `life_exp`. Life expectancies are approximately normally distributed among the states.

``` r
state_df %>% 
  ggplot(aes(x = life_exp)) +
  geom_histogram(bins = 15)
```

![](p8130_hw5_xy2395_files/figure-markdown_github/unnamed-chunk-5-1.png)

Also show the distribution of other variables.

``` r
state_df %>% select(-life_exp) %>% 
plot_num()
```

![](p8130_hw5_xy2395_files/figure-markdown_github/unnamed-chunk-6-1.png)

Problem 2: Model Building Using Automatic Procedures
====================================================

Fit a model using all the predictors.

``` r
fit_all = lm(life_exp ~ ., data = state_df) 
summary(fit_all)
```

    ## 
    ## Call:
    ## lm(formula = life_exp ~ ., data = state_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1.48895 -0.51232 -0.02747  0.57002  1.49447 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  7.094e+01  1.748e+00  40.586  < 2e-16 ***
    ## population   5.180e-05  2.919e-05   1.775   0.0832 .  
    ## income      -2.180e-05  2.444e-04  -0.089   0.9293    
    ## illiteracy   3.382e-02  3.663e-01   0.092   0.9269    
    ## murder      -3.011e-01  4.662e-02  -6.459 8.68e-08 ***
    ## hs_grad      4.893e-02  2.332e-02   2.098   0.0420 *  
    ## frost       -5.735e-03  3.143e-03  -1.825   0.0752 .  
    ## area        -7.383e-08  1.668e-06  -0.044   0.9649    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.7448 on 42 degrees of freedom
    ## Multiple R-squared:  0.7362, Adjusted R-squared:  0.6922 
    ## F-statistic: 16.74 on 7 and 42 DF,  p-value: 2.534e-10

Backward Elimination
--------------------

``` r
# no area
step1 = update(fit_all, . ~ . -area)
summary(step1)
```

    ## 
    ## Call:
    ## lm(formula = life_exp ~ population + income + illiteracy + murder + 
    ##     hs_grad + frost, data = state_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1.49047 -0.52533 -0.02546  0.57160  1.50374 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  7.099e+01  1.387e+00  51.165  < 2e-16 ***
    ## population   5.188e-05  2.879e-05   1.802   0.0785 .  
    ## income      -2.444e-05  2.343e-04  -0.104   0.9174    
    ## illiteracy   2.846e-02  3.416e-01   0.083   0.9340    
    ## murder      -3.018e-01  4.334e-02  -6.963 1.45e-08 ***
    ## hs_grad      4.847e-02  2.067e-02   2.345   0.0237 *  
    ## frost       -5.776e-03  2.970e-03  -1.945   0.0584 .  
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.7361 on 43 degrees of freedom
    ## Multiple R-squared:  0.7361, Adjusted R-squared:  0.6993 
    ## F-statistic: 19.99 on 6 and 43 DF,  p-value: 5.362e-11

``` r
# no illiteracy
step2 = update(step1, . ~ . -illiteracy)
summary(step2)
```

    ## 
    ## Call:
    ## lm(formula = life_exp ~ population + income + murder + hs_grad + 
    ##     frost, data = state_df)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -1.4892 -0.5122 -0.0329  0.5645  1.5166 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  7.107e+01  1.029e+00  69.067  < 2e-16 ***
    ## population   5.115e-05  2.709e-05   1.888   0.0657 .  
    ## income      -2.477e-05  2.316e-04  -0.107   0.9153    
    ## murder      -3.000e-01  3.704e-02  -8.099 2.91e-10 ***
    ## hs_grad      4.776e-02  1.859e-02   2.569   0.0137 *  
    ## frost       -5.910e-03  2.468e-03  -2.395   0.0210 *  
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.7277 on 44 degrees of freedom
    ## Multiple R-squared:  0.7361, Adjusted R-squared:  0.7061 
    ## F-statistic: 24.55 on 5 and 44 DF,  p-value: 1.019e-11

``` r
# no income
step3 = update(step2, . ~ . -income)
summary(step3)
```

    ## 
    ## Call:
    ## lm(formula = life_exp ~ population + murder + hs_grad + frost, 
    ##     data = state_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1.47095 -0.53464 -0.03701  0.57621  1.50683 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  7.103e+01  9.529e-01  74.542  < 2e-16 ***
    ## population   5.014e-05  2.512e-05   1.996  0.05201 .  
    ## murder      -3.001e-01  3.661e-02  -8.199 1.77e-10 ***
    ## hs_grad      4.658e-02  1.483e-02   3.142  0.00297 ** 
    ## frost       -5.943e-03  2.421e-03  -2.455  0.01802 *  
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.7197 on 45 degrees of freedom
    ## Multiple R-squared:  0.736,  Adjusted R-squared:  0.7126 
    ## F-statistic: 31.37 on 4 and 45 DF,  p-value: 1.696e-12

``` r
# no pupulation
step4 = update(step3, . ~ . -population)
summary(step4)
```

    ## 
    ## Call:
    ## lm(formula = life_exp ~ murder + hs_grad + frost, data = state_df)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -1.5015 -0.5391  0.1014  0.5921  1.2268 
    ## 
    ## Coefficients:
    ##              Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) 71.036379   0.983262  72.246  < 2e-16 ***
    ## murder      -0.283065   0.036731  -7.706 8.04e-10 ***
    ## hs_grad      0.049949   0.015201   3.286  0.00195 ** 
    ## frost       -0.006912   0.002447  -2.824  0.00699 ** 
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.7427 on 46 degrees of freedom
    ## Multiple R-squared:  0.7127, Adjusted R-squared:  0.6939 
    ## F-statistic: 38.03 on 3 and 46 DF,  p-value: 1.634e-12

The model we obtained by stepwise elimination is life\_exp ~ murder + hs\_grad + frost.

Forward Elimination
-------------------

``` r
### Step 1:  Fit simple linear regressions for all variables,look for the variable with lowest p-value
variables_list = names(state_df) 

map(variables_list, ~lm(substitute(life_exp ~ i, list(i = as.name(.x))), data = state_df)) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)")
```

    ## Warning in model.matrix.default(mt, mf, contrasts): the response appeared
    ## on the right-hand side and was dropped

    ## Warning in model.matrix.default(mt, mf, contrasts): problem with term 1 in
    ## model.matrix: no columns are assigned

    ## # A tibble: 7 x 5
    ##   term          estimate  std.error statistic  p.value
    ##   <chr>            <dbl>      <dbl>     <dbl>    <dbl>
    ## 1 population -0.0000205  0.0000433     -0.473 6.39e- 1
    ## 2 income      0.000743   0.000297       2.51  1.56e- 2
    ## 3 illiteracy -1.30       0.257         -5.04  6.97e- 6
    ## 4 murder     -0.284      0.0328        -8.66  2.26e-11
    ## 5 hs_grad     0.0968     0.0195         4.96  9.20e- 6
    ## 6 frost       0.00677    0.00360        1.88  6.60e- 2
    ## 7 area       -0.00000169 0.00000226    -0.748 4.58e- 1

``` r
# Enter first the one with the lowest p-value: murder
forward1 = lm(life_exp ~ murder, data = state_df)
tidy(forward1)
```

    ## # A tibble: 2 x 5
    ##   term        estimate std.error statistic  p.value
    ##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>
    ## 1 (Intercept)   73.0      0.270     270.   4.72e-78
    ## 2 murder        -0.284    0.0328     -8.66 2.26e-11

``` r
### Step 2: Enter the one with the lowest p-value in the rest 

map(.x = variables_list, ~update(forward1, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder")
```

    ## Warning in model.matrix.default(mt, mf, contrasts): the response appeared
    ## on the right-hand side and was dropped

    ## Warning in model.matrix.default(mt, mf, contrasts): problem with term 2 in
    ## model.matrix: no columns are assigned

    ## # A tibble: 6 x 5
    ##   term          estimate  std.error statistic p.value
    ##   <chr>            <dbl>      <dbl>     <dbl>   <dbl>
    ## 1 population  0.0000683  0.0000274      2.49  0.0164 
    ## 2 income      0.000370   0.000197       1.88  0.0666 
    ## 3 illiteracy -0.172      0.281         -0.613 0.543  
    ## 4 hs_grad     0.0439     0.0161         2.72  0.00909
    ## 5 frost      -0.00578    0.00266       -2.17  0.0352 
    ## 6 area        0.00000118 0.00000146     0.806 0.424

``` r
# Enter the one with the lowest p-value: hs_grad
forward2 <- update(forward1, . ~ . + hs_grad)
tidy(forward2)
```

    ## # A tibble: 3 x 5
    ##   term        estimate std.error statistic  p.value
    ##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>
    ## 1 (Intercept)  70.3       1.02       69.2  5.91e-49
    ## 2 murder       -0.237     0.0353     -6.72 2.18e- 8
    ## 3 hs_grad       0.0439    0.0161      2.72 9.09e- 3

``` r
### Step 3: Enter the one with the lowest p-value in the rest 
map(.x = variables_list, ~update(forward2, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder", term != 'hs_grad')
```

    ## Warning in model.matrix.default(mt, mf, contrasts): the response appeared
    ## on the right-hand side and was dropped

    ## Warning in model.matrix.default(mt, mf, contrasts): problem with term 3 in
    ## model.matrix: no columns are assigned

    ## # A tibble: 5 x 5
    ##   term          estimate  std.error statistic p.value
    ##   <chr>            <dbl>      <dbl>     <dbl>   <dbl>
    ## 1 population  0.0000625  0.0000259      2.41  0.0199 
    ## 2 income      0.0000953  0.000239       0.398 0.692  
    ## 3 illiteracy  0.254      0.305          0.833 0.409  
    ## 4 frost      -0.00691    0.00245       -2.82  0.00699
    ## 5 area       -0.00000106 0.00000162    -0.658 0.514

``` r
# Enter the one with the lowest p-value: frost
forward3 <- update(forward2, . ~ . + frost)
tidy(forward3)
```

    ## # A tibble: 4 x 5
    ##   term         estimate std.error statistic  p.value
    ##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>
    ## 1 (Intercept)  71.0       0.983       72.2  5.25e-49
    ## 2 murder       -0.283     0.0367      -7.71 8.04e-10
    ## 3 hs_grad       0.0499    0.0152       3.29 1.95e- 3
    ## 4 frost        -0.00691   0.00245     -2.82 6.99e- 3

``` r
### Step 4: Enter the one with the lowest p-value in the rest 
map(.x = variables_list, ~update(forward3, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder", term != 'hs_grad', term != 'frost')
```

    ## Warning in model.matrix.default(mt, mf, contrasts): the response appeared
    ## on the right-hand side and was dropped

    ## Warning in model.matrix.default(mt, mf, contrasts): problem with term 4 in
    ## model.matrix: no columns are assigned

    ## # A tibble: 4 x 5
    ##   term           estimate  std.error statistic p.value
    ##   <chr>             <dbl>      <dbl>     <dbl>   <dbl>
    ## 1 population  0.0000501   0.0000251      2.00   0.0520
    ## 2 income      0.000127    0.000223       0.571  0.571 
    ## 3 illiteracy -0.182       0.328         -0.554  0.582 
    ## 4 area       -0.000000329 0.00000154    -0.214  0.832

``` r
# P-value of all new added variables are larger than 0.05, which means that they 
# are not significant predictor, and we stop here.

# The model we obtained is life_exp ~ murder + hs_grad + frost
forward_fit = lm(life_exp ~ murder + hs_grad + frost, data = state_df)
summary(forward_fit)
```

    ## 
    ## Call:
    ## lm(formula = life_exp ~ murder + hs_grad + frost, data = state_df)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -1.5015 -0.5391  0.1014  0.5921  1.2268 
    ## 
    ## Coefficients:
    ##              Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) 71.036379   0.983262  72.246  < 2e-16 ***
    ## murder      -0.283065   0.036731  -7.706 8.04e-10 ***
    ## hs_grad      0.049949   0.015201   3.286  0.00195 ** 
    ## frost       -0.006912   0.002447  -2.824  0.00699 ** 
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.7427 on 46 degrees of freedom
    ## Multiple R-squared:  0.7127, Adjusted R-squared:  0.6939 
    ## F-statistic: 38.03 on 3 and 46 DF,  p-value: 1.634e-12

The model we obtained by forward elimination is life\_exp ~ murder + hs\_grad + frost.

Stepwise Regreession
--------------------

``` r
step(fit_all, direction = 'backward') %>% 
  summary
```

    ## Start:  AIC=-22.18
    ## life_exp ~ population + income + illiteracy + murder + hs_grad + 
    ##     frost + area
    ## 
    ##              Df Sum of Sq    RSS     AIC
    ## - area        1    0.0011 23.298 -24.182
    ## - income      1    0.0044 23.302 -24.175
    ## - illiteracy  1    0.0047 23.302 -24.174
    ## <none>                    23.297 -22.185
    ## - population  1    1.7472 25.044 -20.569
    ## - frost       1    1.8466 25.144 -20.371
    ## - hs_grad     1    2.4413 25.738 -19.202
    ## - murder      1   23.1411 46.438  10.305
    ## 
    ## Step:  AIC=-24.18
    ## life_exp ~ population + income + illiteracy + murder + hs_grad + 
    ##     frost
    ## 
    ##              Df Sum of Sq    RSS     AIC
    ## - illiteracy  1    0.0038 23.302 -26.174
    ## - income      1    0.0059 23.304 -26.170
    ## <none>                    23.298 -24.182
    ## - population  1    1.7599 25.058 -22.541
    ## - frost       1    2.0488 25.347 -21.968
    ## - hs_grad     1    2.9804 26.279 -20.163
    ## - murder      1   26.2721 49.570  11.569
    ## 
    ## Step:  AIC=-26.17
    ## life_exp ~ population + income + murder + hs_grad + frost
    ## 
    ##              Df Sum of Sq    RSS     AIC
    ## - income      1     0.006 23.308 -28.161
    ## <none>                    23.302 -26.174
    ## - population  1     1.887 25.189 -24.280
    ## - frost       1     3.037 26.339 -22.048
    ## - hs_grad     1     3.495 26.797 -21.187
    ## - murder      1    34.739 58.041  17.456
    ## 
    ## Step:  AIC=-28.16
    ## life_exp ~ population + murder + hs_grad + frost
    ## 
    ##              Df Sum of Sq    RSS     AIC
    ## <none>                    23.308 -28.161
    ## - population  1     2.064 25.372 -25.920
    ## - frost       1     3.122 26.430 -23.877
    ## - hs_grad     1     5.112 28.420 -20.246
    ## - murder      1    34.816 58.124  15.528

    ## 
    ## Call:
    ## lm(formula = life_exp ~ population + murder + hs_grad + frost, 
    ##     data = state_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -1.47095 -0.53464 -0.03701  0.57621  1.50683 
    ## 
    ## Coefficients:
    ##               Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  7.103e+01  9.529e-01  74.542  < 2e-16 ***
    ## population   5.014e-05  2.512e-05   1.996  0.05201 .  
    ## murder      -3.001e-01  3.661e-02  -8.199 1.77e-10 ***
    ## hs_grad      4.658e-02  1.483e-02   3.142  0.00297 ** 
    ## frost       -5.943e-03  2.421e-03  -2.455  0.01802 *  
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 0.7197 on 45 degrees of freedom
    ## Multiple R-squared:  0.736,  Adjusted R-squared:  0.7126 
    ## F-statistic: 31.37 on 4 and 45 DF,  p-value: 1.696e-12

The model we obtained by stepwise regression is life\_exp ~ population + murder + hs\_grad + frost.

### Questions

1.  Do the procedures generate the same model?

Forward elimination and backward elimination generated the same model: `life_exp ~ murder + hs_grad + frost`. However, stepwise regression generated a larger model with one more predictor, `population`.

1.  Is there any variable a close call? What was your decision: keep or discard? Provide arguments for your choice. (Note: this question might have more or less relevance depending on the ‘subset’ you choose).

The variable `population` is a close call, with p-value = 0.052 ∼0.05. I would keep it, because its p-value is quite close to 0.05. Also, adding 'population' contributes to the goodness of fit by increasing the adjusted R2 from 0.6939 to 0.7126.

1.  Is there any association between ‘Illiteracy’ and ‘HS graduation rate’? Does your ‘subset’ contain both?

The Pearson correlation coefficient between ‘Illiteracy’ and ‘HS graduation rate’ is -0.66, indicating a moderate association. My subset only contains one of them. ‘Illiteracy’ is not included.

``` r
skimr::skim(state_df)
```

    ## Skim summary statistics
    ##  n obs: 50 
    ##  n variables: 8 
    ## 
    ## ── Variable type:numeric ───────────────────────────────────────────────────────────────────────────────────────
    ##    variable missing complete  n     mean       sd      p0      p25
    ##        area       0       50 50 70735.88 85327.3  1049    36985.25
    ##       frost       0       50 50   104.46    51.98    0       66.25
    ##     hs_grad       0       50 50    53.11     8.08   37.8     48.05
    ##  illiteracy       0       50 50     1.17     0.61    0.5      0.62
    ##      income       0       50 50  4435.8    614.47 3098     3992.75
    ##    life_exp       0       50 50    70.88     1.34   67.96    70.12
    ##      murder       0       50 50     7.38     3.69    1.4      4.35
    ##  population       0       50 50  4246.42  4464.49  365     1079.5 
    ##       p50      p75     p100     hist
    ##  54277    81162.5  566432   ▇▃▁▁▁▁▁▁
    ##    114.5    139.75    188   ▃▂▃▂▅▇▃▅
    ##     53.25    59.15     67.3 ▅▂▂▃▇▇▃▂
    ##      0.95     1.58      2.8 ▇▃▃▂▂▂▁▁
    ##   4519     4813.5    6315   ▁▅▅▇▆▃▁▁
    ##     70.67    71.89     73.6 ▂▂▁▇▃▅▃▂
    ##      6.85    10.67     15.1 ▆▃▅▃▂▇▂▁
    ##   2838.5   4968.5   21198   ▇▅▁▁▁▁▁▁
