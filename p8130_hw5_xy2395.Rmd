---
title: "p8130_hw5_xy2395"
author: "Jack Yan"
date: "11/30/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(faraway) 
library(patchwork)
library(funModeling)
library(broom)
library(leaps)
```

# Dataset Description

`state.x77` is a matrix with 50 rows and 8 columns giving the following statistics in the respective columns:

*  Population: population estimate as of July 1, 1975.
*  Income: per capita income (1974).
*  Illiteracy: illiteracy (1970, percent of population).
*  Life Exp: life expectancy in years (1969–71).
*  Murder: murder and non-negligent manslaughter rate per 100,000 population (1976).
*  HS Grad: percent high-school graduates (1970).
*  Frost: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city.
*  Area: land area in square miles.

```{r}
# Load the data
state_df = 
  state.x77 %>% 
  as.tibble() %>% 
  janitor::clean_names() %>% 
  select(life_exp, everything())
  
# state_df %>% skimr::skim()
```

# Part 1: Exploratory Data Analysis

## Descriptive Statistics

Descriptive Statistics for all the variables are shown below. All the variables are continuous.

```{r}
# Build a function to generate descriptive statistics for continuous variables
summary_continuous = function(variable){
  data_frame(
    mean = mean(variable),
    sd = sd(variable),
    median = median(variable),
    maximum = max(variable),
    minimum = min(variable),
    IQR = IQR(variable)
  )
}

# Generate descriptive statistics
map(state_df, summary_continuous) %>% 
  bind_rows() %>% 
  mutate(variable = names(state_df)) %>% 
  select(variable, everything()) %>% 
  knitr::kable(digits = 2, 
               caption = "Descriptive statistics of continuous variables")
```

Also show the correlation matrix to check potential correlations. `life_exp` and `murder` are highly correlated. `illiteracy` and `murder` are correlated as well.

```{r}
cor(state_df) %>% 
   knitr::kable(digits = 2)
```

## Plots

First, plot the highly correlated variables.

```{r}
plot(state_df)

murder_illiteracy_points = 
state_df %>% 
  ggplot(aes(x = murder, y = illiteracy))+
    geom_point()
murder_lifeexp_points = 
state_df %>% 
  ggplot(aes(x = murder, y = life_exp))+
    geom_point()
murder_illiteracy_points + murder_lifeexp_points
```

Then check the normality of the outcome, `life_exp`. Life expectancies are approximately normally distributed among the states. 

```{r}
state_df %>% 
  ggplot(aes(x = life_exp)) +
  geom_histogram(bins = 15)
```

Also show the distribution of other variables.

```{r}
state_df %>% select(-life_exp) %>% 
plot_num()
```


# Part 2: Model Building Using Automatic Procedures

Fit a model using all the predictors.

```{r}
fit_all = lm(life_exp ~ ., data = state_df) 
summary(fit_all)
```

## Backward Elimination

```{r}
# no area
step1 = update(fit_all, . ~ . -area)
summary(step1)

# no illiteracy
step2 = update(step1, . ~ . -illiteracy)
summary(step2)

# no income
step3 = update(step2, . ~ . -income)
summary(step3)

# no pupulation
step4 = update(step3, . ~ . -population)
summary(step4)
```

The model we obtained by stepwise elimination is life_exp ~ murder + hs_grad + frost.

## Forward Elimination

```{r}
### Step 1:  Fit simple linear regressions for all variables,look for the variable with lowest p-value
variables_list = names(state_df) 

map(variables_list, ~lm(substitute(life_exp ~ i, list(i = as.name(.x))), data = state_df)) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)")
  
# Enter first the one with the lowest p-value: murder
forward1 = lm(life_exp ~ murder, data = state_df)
tidy(forward1)

### Step 2: Enter the one with the lowest p-value in the rest 

map(.x = variables_list, ~update(forward1, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder")

# Enter the one with the lowest p-value: hs_grad
forward2 <- update(forward1, . ~ . + hs_grad)
tidy(forward2)

### Step 3: Enter the one with the lowest p-value in the rest 
map(.x = variables_list, ~update(forward2, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder", term != 'hs_grad')

# Enter the one with the lowest p-value: frost
forward3 <- update(forward2, . ~ . + frost)
tidy(forward3)

### Step 4: Enter the one with the lowest p-value in the rest 
map(.x = variables_list, ~update(forward3, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder", term != 'hs_grad', term != 'frost')

# P-value of all new added variables are larger than 0.05, which means that they 
# are not significant predictor, and we stop here.

# The model we obtained is life_exp ~ murder + hs_grad + frost
forward_fit = lm(life_exp ~ murder + hs_grad + frost, data = state_df)
summary(forward_fit)
```

The model we obtained by forward elimination is life_exp ~ murder + hs_grad + frost.

## Stepwise Regreession

```{r}
step(fit_all, direction = 'backward') %>% 
  summary
```

The model we obtained by stepwise regression is life_exp ~ population + murder + hs_grad + frost.

### Questions

a) Do the procedures generate the same model?

Forward elimination and backward elimination generated the same model: `life_exp ~ murder + hs_grad + frost`. However, stepwise regression generated a larger model with one more predictor, `population`.

b) Is there any variable a close call? What was your decision: keep or discard? Provide
arguments for your choice. (Note: this question might have more or less relevance depending
on the ‘subset’ you choose).

The variable `population` is a close call, with p-value = 0.052 $\sim 0.05$. I would keep it, because its p-value is quite close to 0.05. This model has a better AIC than a smaller model. Also, adding 'population' contributes to the goodness of fit by increasing the adjusted R2 from 0.6939 to 0.7126. 
 
c) Is there any association between ‘Illiteracy’ and ‘HS graduation rate’? Does your ‘subset’
contain both?

The Pearson correlation coefficient between ‘Illiteracy’ and ‘HS graduation rate’ is -0.66, indicating a moderate association. My subset only contains one of them. ‘Illiteracy’ is not included.

# Part 3: Criterion-based Procedures

```{r}

# Leaps function provides all-subsets analysis

# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = state_df %>% select(-life_exp), y = state_df[[1]], nbest = 1, method = "Cp")

# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = state_df %>% select(-life_exp), y = state_df[[1]], nbest = 1, method = "adjr2")

# Summary of models for each size (one model per size)
b = regsubsets(life_exp ~ ., data = state_df)
rs <- summary(b) 

# Plots of Cp and Adj-R2 as functions of parameters
plot_cp = 
  tibble(x = 1:7, y = rs$cp) %>% 
  ggplot(aes(x = x, y = y)) +
    geom_point() +
    labs(x = "# predictors", y = "Cp")
plot_adjr2 = 
  tibble(x = 1:7, y = rs$adjr2) %>% 
  ggplot(aes(x = x, y = y)) +
    geom_point() +
    labs(x = "# predictors", y = "Adjusted R2")

plot_cp + plot_adjr2
rs
```

Based on the Cp and adjusted R2 criterion, I would choose the 4-predictor model. The best 4-predictor model is life_exp ~ population + murder + hs_grad + frost. It has the highest adjusted R2 and the lowest Cp value.

# Part 4: Diagnostics

The models chosen from Part 2 and 3 are the same: life_exp ~ population + murder + hs_grad + frost, so we will use this one as our final model.

```{r}
final_fit = lm(life_exp ~ population + murder + hs_grad + frost, data = state_df)
par(mfrow=c(2,2))
plot(final_fit)
```

a) Identify any leverage and/or influential points and take appropriate measures.



b) Check the model assumptions.



