---
title: "p8130_hw5_xy2395"
author: "Jack Yan"
date: "11/30/2018"
output: github_document
---

```{r setup, message = F}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(faraway) 
library(patchwork)
library(funModeling)
library(broom)
library(leaps)
library(modelr)
library(ggridges)
set.seed(1)
```

# Dataset Description

`state.x77` is a matrix with 50 rows and 8 columns giving the following statistics in the respective columns:

*  Population: population estimate as of July 1, 1975.
*  Income: per capita income (1974).
*  Illiteracy: illiteracy (1970, percent of population).
*  Life Exp: life expectancy in years (1969–71).
*  Murder: murder and non-negligent manslaughter rate per 100,000 population (1976).
*  HS Grad: percent high-school graduates (1970).
*  Frost: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city.
*  Area: land area in square miles.

```{r}
# Load the data
state_df = 
  state.x77 %>% 
  as.tibble() %>% 
  janitor::clean_names() %>% 
  select(life_exp, everything())
```

# Part 1: Exploratory Data Analysis

## Descriptive Statistics
Build a function to generate descriptive statistics for continuous variables.
```{r}
summary_continuous = function(variable){
  data_frame(
    mean = mean(variable),
    sd = sd(variable),
    median = median(variable),
    maximum = max(variable),
    minimum = min(variable),
    IQR = IQR(variable)
  )
}
```

Descriptive Statistics for all the variables are shown below. All the variables are continuous.

```{r}
# Generate descriptive statistics
map(state_df, summary_continuous) %>% 
  bind_rows() %>% 
  mutate(variable = names(state_df)) %>% 
  select(variable, everything()) %>% 
  knitr::kable(digits = 2, 
               caption = "Descriptive statistics of continuous variables")
```

Also show the correlation matrix to check potential correlations. `life_exp` and `murder` are highly correlated. `illiteracy` and `murder` are correlated as well.

```{r}
# generate the correlation matrix
cor(state_df) %>% 
   knitr::kable(digits = 2,
                caption = "Correlation matrix for all variables")
```

## Plots

First, plot the highly correlated variables.

```{r}
plot(state_df)

murder_illiteracy_points = 
state_df %>% 
  ggplot(aes(x = murder, y = illiteracy)) +
    geom_point()
murder_lifeexp_points = 
state_df %>% 
  ggplot(aes(x = murder, y = life_exp)) +
    geom_point()
murder_illiteracy_points + murder_lifeexp_points
```

`life_exp` and `murder` are highly correlated. `illiteracy` and `murder` are correlated as well.


Then check the normality of the outcome, `life_exp`. Life expectancies of the states are approximately normally distributed . 

```{r}
state_df %>% 
  ggplot(aes(x = life_exp)) +
  geom_histogram(bins = 15)
```

Also show the distribution of other variables.

```{r}
state_df %>% select(-life_exp) %>% 
plot_num()
```


# Part 2: Model Building Using Automatic Procedures


## Backward Elimination
Fit a model using all the predictors. 

```{r}
fit_all = lm(life_exp ~ ., data = state_df) 
summary(fit_all) %>% 
  tidy %>% 
  arrange(p.value)
```

Do the backward elimination. Based on the result above including all the predictors, `area` has the highest p-value 0.9649 > 0.05, and is thus eliminated in the first step.

```{r}
# no area
step1 = update(fit_all, . ~ . -area)
summary(step1) %>% 
  tidy %>% 
  arrange(p.value)
```

Delete `illiteracy` with the highest p-value 0.9340 > 0.05.
```{r}
# no illiteracy
step2 = update(step1, . ~ . -illiteracy)
summary(step2) %>% 
  tidy %>% 
  arrange(p.value)
```

Delete `income` with the highest p-value 0.9153 > 0.05.
```{r}
# no income
step3 = update(step2, . ~ . -income)
summary(step3) %>% 
  tidy %>% 
  arrange(p.value)
```

Delete `population` with the highest p-value 0.05201 > 0.05.
```{r}
# no population
step4 = update(step3, . ~ . -population)
summary(step4) %>% glance
summary(step4) %>% 
  tidy %>% 
  arrange(p.value)

```

Now each predictor remaining in the model has a p-value < 0.05, so we stop here. The model we obtained by stepwise elimination is life_exp ~ murder + hs_grad + frost.

## Forward Elimination

Fit simple linear regressions for all variables and look for the variable with the lowest p-value. Shown below is a summary of each variable fitted in the simple linear regression as the only predictor.

```{r, warning=F}
variables_list = names(state_df) 

map(variables_list, ~lm(substitute(life_exp ~ i, list(i = as.name(.x))), data = state_df)) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)") %>% 
  arrange(p.value)
```

So we first enter the one with the lowest p-value 2.26e-11 < 0.05: `murder`. 

```{r, warning=F}
forward1 = lm(life_exp ~ murder, data = state_df)
tidy(forward1)
```
Add another one predictor to the simple linear regression model. Do this for all the rest predictors. Shown below is a summary of p-value for each variable added separately to the model as an additional predictor.
```{r, warning=F}
map(.x = variables_list, ~update(forward1, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder") %>% 
  arrange(p.value)
```
Enter the one with the lowest p-value 0.00909: `hs_grad`.
```{r, warning=F}
forward2 <- update(forward1, . ~ . + hs_grad)
tidy(forward2)
```
Add another one predictor to the model. Do this for all the rest predictors. Shown below is a summary of p-value for each variable added separately to the model as an additional predictor.
```{r, warning=F}
map(.x = variables_list, ~update(forward2, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder", term != 'hs_grad') %>% 
  arrange(p.value)
```

Enter the one with the lowest p-value 0.00699: `frost`.
```{r, warning=F}
forward3 <- update(forward2, . ~ . + frost)
tidy(forward3)
```
Add another one predictor to the model. Do this for all the rest predictors. Shown below is a summary of p-value for each variable added separately to the model as an additional predictor.
```{r, warning=F}
map(.x = variables_list, ~update(forward3, substitute(. ~ . + i, list(i = as.name(.x))))) %>% 
  map_df(., tidy) %>% 
  filter(term != "(Intercept)", term != "murder", term != 'hs_grad', term != 'frost') %>% 
  arrange(p.value)
```
P-value of all new added variables are larger than 0.05, which means that they are not significant predictor, so we stop here.
```{r, warning=F}
forward_fit = lm(life_exp ~ murder + hs_grad + frost, data = state_df)
summary(forward_fit) %>% tidy()
summary(forward_fit) %>% glance()
```

The model we obtained by forward elimination is life_exp ~ murder + hs_grad + frost.

## Stepwise Regreession

```{r}
step(fit_all, direction = 'backward') %>% 
  summary()
```

The model we obtained by stepwise regression is life_exp ~ population + murder + hs_grad + frost.

### Questions

a) Do the procedures generate the same model?

Forward elimination and backward elimination generated the same model: `life_exp ~ murder + hs_grad + frost`. However, stepwise regression generated a larger model with one more predictor, `population`.

b) Is there any variable a close call? What was your decision: keep or discard? Provide
arguments for your choice. (Note: this question might have more or less relevance depending
on the ‘subset’ you choose).

The variable `population` is a close call, with p-value = 0.052 $\sim 0.05$. I would keep it, because its p-value is quite close to 0.05. This model has a better AIC than a smaller model. Also, adding 'population' contributes to the goodness of fit by increasing the adjusted R2 from 0.6939 to 0.7126. 
 
c) Is there any association between ‘Illiteracy’ and ‘HS graduation rate’? Does your ‘subset’
contain both?

The Pearson correlation coefficient between ‘Illiteracy’ and ‘HS graduation rate’ is -0.66, indicating a moderate association. My subset only contains one of them. ‘Illiteracy’ is not included.

# Part 3: Criterion-based Procedures

```{r}

# Leaps function provides all-subsets analysis

# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = state_df %>% select(-life_exp), y = state_df[[1]], nbest = 1, method = "Cp")

# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = state_df %>% select(-life_exp), y = state_df[[1]], nbest = 1, method = "adjr2")

# Summary of models for each size (one model per size)
b = regsubsets(life_exp ~ ., data = state_df)
rs <- summary(b) 

# Plots of Cp and Adj-R2 as functions of parameters
plot_cp = 
  tibble(x = 1:7, y = rs$cp) %>% 
  ggplot(aes(x = x, y = y)) +
    geom_point() +
    labs(x = "# predictors", y = "Cp")
plot_adjr2 = 
  tibble(x = 1:7, y = rs$adjr2) %>% 
  ggplot(aes(x = x, y = y)) +
    geom_point() +
    labs(x = "# predictors", y = "Adjusted R2")

plot_cp + plot_adjr2
rs
```

Based on the Cp and adjusted R2 criterion, I would choose the 4-predictor model. The best 4-predictor model is life_exp ~ population + murder + hs_grad + frost. It has the highest adjusted R2 and the lowest Cp value.

# Part 4: Diagnostics

The models chosen from Part 2 and 3 are the same: life_exp ~ population + murder + hs_grad + frost, so we will use this one as our final model.

```{r}
final_fit = lm(life_exp ~ population + murder + hs_grad + frost, data = state_df)
par(mfrow = c(2,2))
plot(final_fit)

influence.measures(final_fit)

```

a) Identify any leverage and/or influential points and take appropriate measures.

Based on Cook's distance, Observation 11 is a potentially influential observation. Remove this point and fit the model again.

```{r}
state_df_no_11 = 
  state_df %>% 
  mutate(index = 1:nrow(state_df)) %>% 
  filter(!index %in% c(11)) 

final_fit_no_11 = lm(life_exp ~ population + murder + hs_grad + frost, data = state_df_no_11)
par(mfrow=c(2,2))
plot(final_fit_no_11)
```

b) Check the model assumptions.

The residuals are spread equally along the fitted values. We can assume that the residuals have mean 0 and equal variance, and are independent of each other. The QQ plot shows a minor deviate from normal distribubtion at the tail, which may be caused by outliers. 


# Model Validation
Using the ‘final’ model chosen in part 4, focus on MSE to test the model predictive ability.

a) Use a 10-fold cross-validation (10 repeats).

```{r}

cv_df = 
  crossv_kfold(state_df, 10) %>% 
  mutate(model = map(train, ~lm(life_exp ~ population + murder + hs_grad + frost, data = .x))) %>% 
  mutate(mse = map2_dbl(model, test, ~mse(model = .x, data = .y))) 
```
Below shows the 10 mse's of the 10-fold cross-validation.
```{r}
cv_df$mse
summary_continuous(cv_df$mse)
cv_df %>% 
  ggplot(aes(x = mse, y = "model")) +
  geom_density_ridges()
```
This is the summary of the 10-fold cross-validation. `summary_continuous` function was writen in Part 1.

b) Bootstrapping

```{r}
fit_final = lm(life_exp ~ population + murder + hs_grad + frost, data = state_df)

bootstrap_df = 
  state_df %>% 
  add_predictions(fit_final) %>% 
  mutate(resid = life_exp - pred) 

bootstrap = function(x){
  bootstrap_df_new =
    bootstrap_df %>% 
    mutate(y_new = pred + sample(bootstrap_df$resid, replace = TRUE)) 
  model = lm(y_new ~ population + murder + hs_grad + frost, data = bootstrap_df_new)
  mse(model, bootstrap_df_new)
}

result_df_10 = 
  map(1:10, bootstrap) %>% 
  c() %>%  as.numeric %>% as.tibble()

result_df_1000 = 
  map(1:1000, bootstrap) %>% 
  c() %>%  as.numeric %>% as.tibble()

summary_continuous(result_df_10$value)
summary_continuous(result_df_1000$value)
```
Here shows the discriptive statistics of the 10X and 1000X bootstrap. 

```{r}
plot_10x =
result_df_10 %>% 
  ggplot(aes(y = "density", x = value)) +
    geom_density_ridges() +
    labs(title = "Distribution of MSE (10 times)")
plot_1000x =
result_df_1000 %>% 
  ggplot(aes(y="density", x = value)) +
    geom_density_ridges() +
    labs(title = "Distribution of MSE (1000 times)")

plot_10x + plot_1000x
```

The plots show the distribution of MSEs from the 10-time and 1000-time bootstrapping.

c) Summary

```{r}
bind_rows(
  summary_continuous(cv_df$mse),
  summary_continuous(result_df_10$value),
  summary_continuous(result_df_1000$value)
) %>% 
  mutate(method = c('CV-10fold', 'bootstrap-10X','bootstrap-1000X')) %>% 
  select(method, everything()) %>% 
  knitr::kable(digits = 2)
```

Although the medians of the two methods are similar (0.41 vs 0.42), their means and standard deviations are quite different. For bootstrapping, The means and SD's of MSE are similar in the 10 trials and 1000 trials, showing reliability of this method even with small amount of trials. On the other hand, the standard deviation for the Cross Validation method is significantly higher, which indicates a low stability of this method, especially when k (# of folds) is small. Therefore, I would recommend *bootstrapping* for assessing model performance.


